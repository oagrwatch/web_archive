#!/usr/bin/env python3
"""
Wayback Machine Content Exporter â€” Advanced (Trafilatura + Readability + Boilerplate removal)

Features:
- Query Wayback CDX API for snapshots of a given domain/path
- Optional temporal filtering (user inputs dates in DD/MM/YYYY)
- Option to collect all snapshots or a user-specified number
- Progressive download with tqdm progress bar
- SSL "soft" fallback: try normal verify, then retry with verify=False on SSL errors
- Intermediate chunked saves every CHUNK_SIZE records (CSV, XLSX, JSON) with RAW content
- Final save that unifies and CLEANS all collected records into final CSV/XLSX/JSON
- Timestamps in output files formatted as DD/MM/YYYY
- Advanced cleaning:
    * primary extraction with trafilatura
    * fallback with readability-lxml
    * fallback with BeautifulSoup
    * post-processing: remove repeated boilerplate lines across pages, date-only lines,
      navigation words (Î”ÎµÎ¯Ï„Îµ, Î±Î½Î±Î»Ï…Ï„Î¹ÎºÎ¬), very short lines, footer/contact blocks
- Graceful handling of KeyboardInterrupt

Usage:
    python wayback_collector_advanced.py

Requires:
    pip install requests pandas beautifulsoup4 tqdm openpyxl trafilatura readability-lxml lxml
"""

import requests
import os
import json
import re
from bs4 import BeautifulSoup
from tqdm import tqdm
from datetime import datetime
import pandas as pd
import urllib3
import trafilatura
from readability import Document
from collections import defaultdict

# suppress insecure request warnings when verify=False used
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# -------------------- Configuration --------------------
OUTPUT_PREFIX = "wayback_export"
CHUNK_SIZE = 500  # change if you want smaller/larger chunks
CDX_BASE = "http://web.archive.org/cdx/search/cdx"

# Boilerplate detection thresholds
BOILERPLATE_MIN_PAGES = 3        # minimal distinct pages a line must appear in to be candidate
BOILERPLATE_RATIO = 0.15        # or appear on >= 15% of pages -> considered boilerplate

# Heuristic thresholds
MIN_LINE_LENGTH = 20            # lines shorter than this (chars) often navigation/junk
MIN_WORDS_LINE = 3              # lines with fewer words than this are often navigation

# Regex for date-only lines (e.g., 26/07/04 or 2004-07-26)
RE_DATE_LIKE = re.compile(r'^(?:\d{1,2}[\/\-\.\s]\d{1,2}[\/\-\.\s]\d{2,4}|\d{4}[\/\-\.\s]\d{1,2}[\/\-\.\s]\d{1,2})$')
RE_EMAIL = re.compile(r'\b[\w\.-]+@[\w\.-]+\.\w+\b')
RE_PHONE = re.compile(r'(\+?\d[\d\-\s\(\)]{5,}\d)')
RE_COPYRIGHT = re.compile(r'Â©|copyright|Î”Î®Î»Ï‰ÏƒÎ·|Î ÏÎ¿ÏƒÏ„Î±ÏƒÎ¯Î±|Î¤Î·Î»:|Fax:|Fax|Î¤Î·Î»Î­Ï†Ï‰Î½Î¿', re.I)
NAV_WORDS = set(['Î”ÎµÎ¯Ï„Îµ', 'Î±Î½Î±Î»Ï…Ï„Î¹ÎºÎ¬', 'Î±Î½Î±Î»Ï…Ï„Î¹ÎºÎ±', 'Î‘ÏÏ‡Î¹ÎºÎ®', 'Î ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ±', 'Read', 'More', 'Â»', 'â€¹', 'â€º', '...'])

# -------------------- Helper functions --------------------

def normalize_domain_input(domain_raw: str) -> str:
    """Normalize the user's domain/path input into a form usable in CDX queries."""
    if not domain_raw:
        return ""
    s = domain_raw.strip()
    if s.startswith("http://"):
        s = s[len("http://"):]
    elif s.startswith("https://"):
        s = s[len("https://"):]
    return s.rstrip('/')


def build_cdx_query(domain_path: str, from_ts: str = None, to_ts: str = None):
    params = {
        'url': f"{domain_path}/*",
        'output': 'json',
        'fl': 'timestamp,original',
        'filter': 'statuscode:200'
    }
    query_parts = [f"url={params['url']}", f"output={params['output']}", f"fl={params['fl']}", f"filter={params['filter']}"]
    if from_ts:
        query_parts.append(f"from={from_ts}")
    if to_ts:
        query_parts.append(f"to={to_ts}")
    query = CDX_BASE + "?" + "&".join(query_parts)
    return query


def parse_date_input_ddmmyyyy(inp: str) -> datetime:
    return datetime.strptime(inp.strip(), "%d/%m/%Y")


def ts_to_readable_date(ts: str) -> str:
    try:
        dt = datetime.strptime(ts[:14], "%Y%m%d%H%M%S")
        return dt.strftime("%d/%m/%Y")
    except Exception:
        return ts


def extract_with_trafilatura(url_or_html: str, is_html=False):
    """Attempt extraction with trafilatura.
    If is_html=True, url_or_html is HTML string; else it's a URL string and we fetch it via trafilatura.fetch_url."""
    try:
        if is_html:
            downloaded = url_or_html
            res = trafilatura.extract(downloaded, include_comments=False, include_tables=False, include_formatting=False)
            meta = trafilatura.extract_metadata(downloaded)
        else:
            downloaded = trafilatura.fetch_url(url_or_html)
            if not downloaded:
                return "", ""
            res = trafilatura.extract(downloaded, include_comments=False, include_tables=False, include_formatting=False)
            meta = trafilatura.extract_metadata(downloaded)
        title = ""
        if meta and hasattr(meta, 'get'):
            title = meta.get('title', '') if meta else ''
        if res:
            return title or "", res.strip()
    except Exception:
        pass
    return "", ""


def extract_with_readability(html: str):
    try:
        doc = Document(html)
        title = doc.short_title() or ""
        summary = doc.summary()
        soup = BeautifulSoup(summary, "html.parser")
        text = soup.get_text(separator="\n")
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return title, "\n".join(lines)
    except Exception:
        return "", ""


def extract_with_bs4(html: str):
    try:
        soup = BeautifulSoup(html, "html.parser")
        title = ""
        if soup.title and soup.title.string:
            title = soup.title.string.strip()
        for t in soup(["script", "style", "noscript"]):
            t.decompose()
        text = soup.get_text(separator="\n")
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return title, "\n".join(lines)
    except Exception:
        return "", ""


def safe_request_get(url: str, timeout: int = 15) -> str:
    try:
        resp = requests.get(url, timeout=timeout)
        resp.raise_for_status()
        return resp.text
    except requests.exceptions.SSLError:
        resp = requests.get(url, timeout=timeout, verify=False)
        resp.raise_for_status()
        return resp.text


def looks_like_date_line(line: str) -> bool:
    return bool(RE_DATE_LIKE.match(line.strip()))


def is_junk_line(line: str) -> bool:
    s = line.strip()
    if not s:
        return True
    # email or phone
    if RE_EMAIL.search(s) or RE_PHONE.search(s):
        return True
    # copyright/contact
    if RE_COPYRIGHT.search(s):
        return True
    # navigation words alone
    if s in NAV_WORDS:
        return True
    # arrows or short UI strings
    if s in ('Â»', 'Â«', 'â€º', 'â€¹', '...'):
        return True
    # date-like lines
    if looks_like_date_line(s):
        return True
    # very short or very few words
    if len(s) < MIN_LINE_LENGTH or len(s.split()) < MIN_WORDS_LINE:
        return True
    return False


def save_chunk_raw(records, chunk_index: int):
    """Save chunk with RAW content (before boilerplate removal)."""
    if not records:
        return
    base = f"{OUTPUT_PREFIX}_chunk_raw_{chunk_index}"
    rows = []
    for r in records:
        rows.append({
            'timestamp': ts_to_readable_date(r.get('timestamp', '')),
            'original_url': r.get('original_url', ''),
            'archive_url': r.get('archive_url', ''),
            'title': r.get('title', ''),
            'raw_content': r.get('raw_content', '')
        })
    df = pd.DataFrame(rows)
    csv_name = base + '.csv'
    xlsx_name = base + '.xlsx'
    json_name = base + '.json'
    df.to_csv(csv_name, index=False, encoding='utf-8')
    df.to_excel(xlsx_name, index=False)
    with open(json_name, 'w', encoding='utf-8') as jf:
        json.dump(rows, jf, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ Î•Î½Î´Î¹Î¬Î¼ÎµÏƒÎ· raw Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· chunk #{chunk_index}: {csv_name}, {xlsx_name}, {json_name}")


def save_final_clean(records):
    """After cleaning, save final CSV/XLSX/JSON with cleaned 'content' field and readable timestamps."""
    if not records:
        print("âš ï¸ Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± Ï„ÎµÎ»Î¹ÎºÎ® Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·.")
        return
    rows = []
    for r in records:
        rows.append({
            'timestamp': ts_to_readable_date(r.get('timestamp', '')),
            'original_url': r.get('original_url', ''),
            'archive_url': r.get('archive_url', ''),
            'title': r.get('title', ''),
            'content': r.get('content', '')
        })
    df = pd.DataFrame(rows)
    csv_name = OUTPUT_PREFIX + '_all_clean.csv'
    xlsx_name = OUTPUT_PREFIX + '_all_clean.xlsx'
    json_name = OUTPUT_PREFIX + '_all_clean.json'
    df.to_csv(csv_name, index=False, encoding='utf-8')
    df.to_excel(xlsx_name, index=False)
    with open(json_name, 'w', encoding='utf-8') as jf:
        json.dump(rows, jf, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ Î¤ÎµÎ»Î¹ÎºÎ® ÎºÎ±Î¸Î±ÏÎ® Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·: {csv_name}, {xlsx_name}, {json_name}")


# -------------------- Main program --------------------

def main():
    print("=== Wayback Machine Content Exporter (Advanced) ===\n")
    user_input = input("ğŸ”— Î Î»Î·ÎºÏ„ÏÎ¿Î»ÏŒÎ³Î·ÏƒÎµ Ï„Î· Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ· (Ï€.Ï‡. example.com Î® www.example.com/path): ").strip()
    if not user_input:
        print("âŒ Î”ÎµÎ½ Î´ÏŒÎ¸Î·ÎºÎµ Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·. ÎˆÎ¾Î¿Î´Î¿Ï‚.")
        return
    domain_path = normalize_domain_input(user_input)

    # date filter
    print("\nÎ˜ÎµÏ‚ Î½Î± Ï€ÎµÏÎ¹Î¿ÏÎ¯ÏƒÎµÎ¹Ï‚ Ï„Î·Î½ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· ÏƒÎµ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿ Ï‡ÏÎ¿Î½Î¹ÎºÏŒ Î´Î¹Î¬ÏƒÏ„Î·Î¼Î±;")
    print("1. ÎŒÏ‡Î¹ â€” ÏŒÎ»Î± Ï„Î± snapshots")
    print("2. ÎÎ±Î¹ â€” Î¸Î± Î´ÏÏƒÏ‰ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚ (DD/MM/YYYY)")
    date_choice = input("ğŸ‘‰ Î•Ï€Î¯Î»ÎµÎ¾Îµ (1 Î® 2): ").strip()

    from_ts = None
    to_ts = None
    if date_choice == '2':
        while True:
            s = input("ğŸ”¹ Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± Î­Î½Î±ÏÎ¾Î·Ï‚ (DD/MM/YYYY): ").strip()
            try:
                dt_s = parse_date_input_ddmmyyyy(s)
                from_ts = dt_s.strftime('%Y%m%d') + '000000'
                break
            except Exception:
                print("âš ï¸ ÎœÎ· Î­Î³ÎºÏ…ÏÎ· Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±. Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Ï€.Ï‡. 01/01/1999")
        while True:
            s = input("ğŸ”¹ Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± Î»Î®Î¾Î·Ï‚ (DD/MM/YYYY): ").strip()
            try:
                dt_e = parse_date_input_ddmmyyyy(s)
                to_ts = dt_e.strftime('%Y%m%d') + '235959'
                if from_ts and int(from_ts) > int(to_ts):
                    print("âš ï¸ Î— Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± Î»Î®Î¾Î·Ï‚ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ Î¼ÎµÏ„Î¬ Ï„Î·Î½ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± Î­Î½Î±ÏÎ¾Î·Ï‚.")
                    continue
                break
            except Exception:
                print("âš ï¸ ÎœÎ· Î­Î³ÎºÏ…ÏÎ· Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±. Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Ï€.Ï‡. 31/12/2015")

    # how many snapshots
    print("\nÎ ÏŒÏƒÎ± snapshots Î¸ÎµÏ‚ Î½Î± ÏƒÏ…Î»Î»ÎµÏ‡Î¸Î¿ÏÎ½;")
    print("1. ÎŒÎ»Î±")
    print("2. Î£Ï…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿Ï‚ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚")
    how_many = input("ğŸ‘‰ Î•Ï€Î¯Î»ÎµÎ¾Îµ (1 Î® 2): ").strip()
    max_snapshots = None
    if how_many == '2':
        while True:
            val = input("ğŸ”¢ Î Î»Î·ÎºÏ„ÏÎ¿Î»ÏŒÎ³Î·ÏƒÎµ Ï€ÏŒÏƒÎ± snapshots Î¸Î­Î»ÎµÎ¹Ï‚ (Ï€.Ï‡. 50): ").strip()
            try:
                n = int(val)
                if n > 0:
                    max_snapshots = n
                    break
            except Exception:
                pass
            print("âš ï¸ Î“ÏÎ¬ÏˆÎµ Î­Î½Î±Î½ Î¸ÎµÏ„Î¹ÎºÏŒ Î±ÎºÎ­ÏÎ±Î¹Î¿ Î±ÏÎ¹Î¸Î¼ÏŒ.")

    cdx_url = build_cdx_query(domain_path, from_ts=from_ts, to_ts=to_ts)
    print(f"\nğŸ” Î•ÏÏÏ„Î·Î¼Î± ÏƒÏ„Î¿ Wayback CDX API...\n   {cdx_url}\n")

    try:
        resp = requests.get(cdx_url, timeout=20)
        resp.raise_for_status()
        raw = resp.json()
    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ Î±Î¯Ï„Î·Î¼Î± CDX API: {e}")
        return

    if len(raw) <= 1:
        print("âš ï¸ Î¤Î¿ CDX API Î´ÎµÎ½ ÎµÏ€Î­ÏƒÏ„ÏÎµÏˆÎµ snapshots Î³Î¹Î± Ï„Î± ÎºÏÎ¹Ï„Î®ÏÎ¹Î± Î±Ï…Ï„Î¬.")
        return

    rows = raw[1:]
    if max_snapshots is not None:
        rows = rows[:max_snapshots]

    total = len(rows)
    print(f"âœ… Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {total} snapshots (Î¸Î± ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ·Î¸Î¿ÏÎ½ Î»Î®ÏˆÎµÎ¹Ï‚).\n")

    all_records = []      # list of dicts with timestamp, original_url, archive_url, title, raw_content, content (cleaned later)
    chunk_buffer = []
    chunk_index = 1

    try:
        for item in tqdm(rows, desc='Î›Î®ÏˆÎ· snapshot ÏƒÎµÎ»Î¯Î´Ï‰Î½', unit='snap'):
            try:
                timestamp, original = item
            except Exception:
                tqdm.write("âš ï¸ Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ· Î¼Î· Î±Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï… CDX entry")
                continue

            archive_url = f"https://web.archive.org/web/{timestamp}/{original}"

            try:
                html = safe_request_get(archive_url, timeout=15)
            except Exception as e:
                tqdm.write(f"âš ï¸ Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ· (Î».Î». Î±Î¯Ï„Î·Î¼Î±) {archive_url} ({e})")
                continue

            # Attempt extraction: trafilatura -> readability -> bs4 fallback
            title, main_text = "", ""
            try:
                # try trafilatura on HTML string
                title, main_text = extract_with_trafilatura(html, is_html=True)
            except Exception:
                title, main_text = "", ""

            if not main_text.strip():
                # try readability
                try:
                    title2, main_text2 = extract_with_readability(html)
                    if main_text2 and len(main_text2) > len(main_text):
                        title = title2 or title
                        main_text = main_text2
                except Exception:
                    pass

            if not main_text.strip():
                # bs4 fallback
                try:
                    title3, main_text3 = extract_with_bs4(html)
                    if main_text3:
                        title = title3 or title
                        main_text = main_text3
                except Exception:
                    pass

            if not main_text.strip():
                tqdm.write(f"âš ï¸ Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ· (ÎºÎµÎ½ÏŒ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿) {archive_url}")
                continue

            # store raw content (pre-clean)
            rec = {
                'timestamp': timestamp,
                'original_url': original,
                'archive_url': archive_url,
                'title': title or "",
                'raw_content': main_text,
                'content': ""   # placeholder for cleaned
            }
            all_records.append(rec)
            chunk_buffer.append(rec)

            # chunk raw save
            if len(chunk_buffer) >= CHUNK_SIZE:
                save_chunk_raw(chunk_buffer, chunk_index)
                chunk_index += 1
                chunk_buffer = []

    except KeyboardInterrupt:
        print("\nâ¹ï¸ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Î´Î¹Î±ÎºÏŒÏ€Î·ÎºÎµ Î±Ï€ÏŒ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·. Î˜Î± Î³Î¯Î½ÎµÎ¹ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï€Î¿Ï… Î­Ï‡Î¿Ï…Î½ ÏƒÏ…Î»Î»ÎµÏ‡Î¸ÎµÎ¯...")

    finally:
        # save remaining raw chunk
        if chunk_buffer:
            save_chunk_raw(chunk_buffer, chunk_index)

    # If no records collected
    if not all_records:
        print("âš ï¸ Î”ÎµÎ½ ÏƒÏ…Î»Î»Î­Ï‡Î¸Î·ÎºÎ±Î½ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚. Î¤Î­Î»Î¿Ï‚.")
        return

    # -------------------- Advanced post-processing / boilerplate detection --------------------
    print("\nğŸ” Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Ï€ÏÎ¿Î·Î³Î¼Î­Î½Î·Ï‚ Î±Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ·Ï‚ boilerplate ÎºÎ±Î¹ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î¿Ï...")

    # Build index: line -> set(page_indices)
    line_pages = defaultdict(set)
    page_lines = []  # list of lists (per page)
    for idx, rec in enumerate(all_records):
        lines = [ln.strip() for ln in rec['raw_content'].splitlines() if ln.strip()]
        page_lines.append(lines)
        unique_lines = set(lines)
        for ln in unique_lines:
            if len(ln) < 3:
                continue
            # normalize some whitespace and punctuation for detection
            nl = re.sub(r'\s+', ' ', ln).strip()
            line_pages[nl].add(idx)

    num_pages = len(all_records)
    # detect boilerplate candidates
    boilerplate_lines = set()
    for ln, pageset in line_pages.items():
        count = len(pageset)
        if count >= BOILERPLATE_MIN_PAGES or (count / num_pages) >= BOILERPLATE_RATIO:
            # also filter out lines that are short but repeat often (menus)
            boilerplate_lines.add(ln)

    # Expand boilerplate patterns by heuristics: small variations (lower/strip punctuation)
    expanded_boilerplate = set(boilerplate_lines)
    for ln in list(boilerplate_lines):
        lnl = ln.lower()
        # also consider stripped punctuation version
        s = re.sub(r'[^\w\s]', '', lnl).strip()
        if s and s != lnl:
            expanded_boilerplate.add(s)

    # Now clean each page: remove boilerplate lines and junk lines
    cleaned_count = 0
    for idx, rec in enumerate(all_records):
        raw = rec['raw_content']
        lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
        cleaned_lines = []
        for ln in lines:
            norm = re.sub(r'\s+', ' ', ln).strip()
            norm_low = norm.lower()
            short_norm = re.sub(r'[^\w\s]', '', norm_low).strip()
            # skip if matches boilerplate (exact or normalized)
            if norm in boilerplate_lines or norm_low in boilerplate_lines or short_norm in expanded_boilerplate:
                continue
            # skip if junk heuristics
            if is_junk_line(norm):
                continue
            cleaned_lines.append(norm)
        # post-processing: merge consecutive short lines if they form sentences?
        # simple join
        final_text = "\n".join(cleaned_lines).strip()
        # if after cleaning the text is very small, fall back to raw but filtered minimal junk removal
        if len(final_text) < 100:
            # try lighter cleaning: remove pure junk lines only
            lite = [ln for ln in lines if not is_junk_line(ln)]
            final_text = "\n".join(lite).strip()
        rec['content'] = final_text
        if final_text:
            cleaned_count += 1

    print(f"âœ… ÎšÎ±Î¸Î±ÏÎ¯ÏƒÏ„Î·ÎºÎ±Î½ ÎºÎµÎ¯Î¼ÎµÎ½Î± Î³Î¹Î± {cleaned_count}/{num_pages} ÏƒÎµÎ»Î¯Î´ÎµÏ‚.")

    # -------------------- Save final cleaned outputs --------------------
    save_final_clean(all_records)
    print(f"\nÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ â€” ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ¬ ÏƒÎµÎ»Î¯Î´ÎµÏ‚ Ï€Î¿Ï… ÏƒÏÎ¸Î·ÎºÎ±Î½: {len(all_records)}")


if __name__ == '__main__':
    main()

