#!/usr/bin/env python3
"""
Wayback Machine Content Exporter (full)

Features:
- Query Wayback CDX API for snapshots of a given domain/path
- Optional temporal filtering (user inputs dates in DD/MM/YYYY)
- Option to collect all snapshots or a user-specified number
- Progressive download with tqdm progress bar
- SSL "soft" fallback: try normal verify, then retry with verify=False on SSL errors
- Intermediate chunked saves every CHUNK_SIZE records (CSV, XLSX, JSON)
- Final save that unifies all collected records into final CSV/XLSX/JSON
- Timestamps in output files formatted as DD/MM/YYYY
- Graceful handling of KeyboardInterrupt

Usage:
    python wayback_collector_full.py

Requires:
    pip install requests pandas beautifulsoup4 tqdm openpyxl

Note: trafilatura/newspaper not used here (this script fetches archived HTML and extracts text via BeautifulSoup).
"""

import requests
import os
import json
from bs4 import BeautifulSoup
from tqdm import tqdm
from datetime import datetime
import pandas as pd
import urllib3

# suppress insecure request warnings when verify=False used
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# -------------------- Configuration --------------------
OUTPUT_PREFIX = "wayback_export"
CHUNK_SIZE = 500  # change if you want smaller/larger chunks
CDX_BASE = "http://web.archive.org/cdx/search/cdx"

# -------------------- Helper functions --------------------

def normalize_domain_input(domain_raw: str) -> str:
    """Normalize the user's domain/path input into a form usable in CDX queries.

    Accepts inputs like:
      example.com
      www.example.com
      example.com/path
      https://example.com
    Returns string without trailing slash and without protocol for CDX usage.
    """
    if not domain_raw:
        return ""
    s = domain_raw.strip()
    # strip protocol if present
    if s.startswith("http://"):
        s = s[len("http://"):]
    elif s.startswith("https://"):
        s = s[len("https://"):]
    # remove trailing slash
    s = s.rstrip('/')
    return s


def build_cdx_query(domain_path: str, from_ts: str = None, to_ts: str = None):
    """Build CDX API URL for given domain/path and optional from/to timestamps.

    The CDX parameters used:
      url={domain_path}/*
      output=json
      fl=timestamp,original
      filter=statuscode:200
      from=YYYYMMDDhhmmss (optional)
      to=YYYYMMDDhhmmss (optional)

    Returns the full URL string.
    """
    params = {
        'url': f"{domain_path}/*",
        'output': 'json',
        'fl': 'timestamp,original',
        'filter': 'statuscode:200'
    }

    # Build base query string
    query_parts = [f"url={params['url']}", f"output={params['output']}", f"fl={params['fl']}", f"filter={params['filter']}"]
    if from_ts:
        query_parts.append(f"from={from_ts}")
    if to_ts:
        query_parts.append(f"to={to_ts}")

    query = CDX_BASE + "?" + "&".join(query_parts)
    return query


def parse_date_input_ddmmyyyy(inp: str) -> datetime:
    """Parse a date string in DD/MM/YYYY and return a datetime.date object (at midnight).

    Raises ValueError on invalid input.
    """
    return datetime.strptime(inp.strip(), "%d/%m/%Y")


def ts_to_readable_date(ts: str) -> str:
    """Convert Wayback timestamp YYYYMMDDhhmmss to DD/MM/YYYY string.
    If conversion fails, return original string.
    """
    try:
        dt = datetime.strptime(ts[:14], "%Y%m%d%H%M%S")
        return dt.strftime("%d/%m/%Y")
    except Exception:
        return ts


def extract_text_from_html(html: str) -> (str, str):
    """Extract title and cleaned text from HTML using BeautifulSoup.

    Returns (title, text). Empty strings if nothing found.
    """
    if not html:
        return "", ""
    soup = BeautifulSoup(html, "html.parser")
    title = ""
    try:
        if soup.title and soup.title.string:
            title = soup.title.string.strip()
    except Exception:
        title = ""

    # remove script/style/noscript elements
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    text = soup.get_text(separator="\n")
    # collapse and strip
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    cleaned = "\n".join(lines)
    return title, cleaned


def safe_request_get(url: str, timeout: int = 15) -> str:
    """Try to GET a URL. On SSL errors, retry with verify=False. Returns response.text or raises.
    """
    try:
        resp = requests.get(url, timeout=timeout)
        resp.raise_for_status()
        return resp.text
    except requests.exceptions.SSLError:
        # retry with SSL verify disabled (soft fallback)
        resp = requests.get(url, timeout=timeout, verify=False)
        resp.raise_for_status()
        return resp.text


def save_chunk(data_records, chunk_index: int):
    """Save a chunk (list of records) to CSV/XLSX/JSON files.

    Each record is a dict with keys: timestamp, original_url, archive_url, title, content
    Timestamps will be converted to DD/MM/YYYY in output.
    """
    if not data_records:
        return
    base = f"{OUTPUT_PREFIX}_chunk_{chunk_index}"
    # convert to DataFrame with readable dates
    rows = []
    for r in data_records:
        rows.append({
            'timestamp': ts_to_readable_date(r.get('timestamp', '')),
            'original_url': r.get('original_url', ''),
            'archive_url': r.get('archive_url', ''),
            'title': r.get('title', ''),
            'content': r.get('content', '')
        })
    df = pd.DataFrame(rows)
    csv_name = base + '.csv'
    xlsx_name = base + '.xlsx'
    json_name = base + '.json'
    df.to_csv(csv_name, index=False, encoding='utf-8')
    df.to_excel(xlsx_name, index=False)
    with open(json_name, 'w', encoding='utf-8') as jf:
        json.dump(rows, jf, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ Î•Î½Î´Î¹Î¬Î¼ÎµÏƒÎ· Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· chunk #{chunk_index}: {csv_name}, {xlsx_name}, {json_name}")


def save_final(all_records):
    """Save final unified output files with readable dates."""
    if not all_records:
        print("âš ï¸ Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± Ï„ÎµÎ»Î¹ÎºÎ® Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·.")
        return
    rows = []
    for r in all_records:
        rows.append({
            'timestamp': ts_to_readable_date(r.get('timestamp', '')),
            'original_url': r.get('original_url', ''),
            'archive_url': r.get('archive_url', ''),
            'title': r.get('title', ''),
            'content': r.get('content', '')
        })
    df = pd.DataFrame(rows)
    csv_name = OUTPUT_PREFIX + '_all.csv'
    xlsx_name = OUTPUT_PREFIX + '_all.xlsx'
    json_name = OUTPUT_PREFIX + '_all.json'
    df.to_csv(csv_name, index=False, encoding='utf-8')
    df.to_excel(xlsx_name, index=False)
    with open(json_name, 'w', encoding='utf-8') as jf:
        json.dump(rows, jf, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ Î¤ÎµÎ»Î¹ÎºÎ® Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·: {csv_name}, {xlsx_name}, {json_name}")

# -------------------- Main program --------------------

def main():
    print("=== Wayback Machine Content Exporter (with date filter & chunks) ===\n")

    user_input = input("ğŸ”— Î Î»Î·ÎºÏ„ÏÎ¿Î»ÏŒÎ³Î·ÏƒÎµ Ï„Î· Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ· (Ï€.Ï‡. example.com Î® www.example.com/path): ").strip()
    if not user_input:
        print("âŒ Î”ÎµÎ½ Î´ÏŒÎ¸Î·ÎºÎµ Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·. ÎˆÎ¾Î¿Î´Î¿Ï‚.")
        return
    domain_path = normalize_domain_input(user_input)

    # ask about date filtering
    print("\nÎ˜ÎµÏ‚ Î½Î± Ï€ÎµÏÎ¹Î¿ÏÎ¯ÏƒÎµÎ¹Ï‚ Ï„Î·Î½ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· ÏƒÎµ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿ Ï‡ÏÎ¿Î½Î¹ÎºÏŒ Î´Î¹Î¬ÏƒÏ„Î·Î¼Î±;")
    print("1. ÎŒÏ‡Î¹ â€” ÏŒÎ»Î± Ï„Î± snapshots")
    print("2. ÎÎ±Î¹ â€” Î¸Î± Î´ÏÏƒÏ‰ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯ÎµÏ‚ (DD/MM/YYYY)")
    date_choice = input("ğŸ‘‰ Î•Ï€Î¯Î»ÎµÎ¾Îµ (1 Î® 2): ").strip()

    from_ts = None
    to_ts = None
    if date_choice == '2':
        # loop for valid start date
        while True:
            s = input("ğŸ”¹ Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± Î­Î½Î±ÏÎ¾Î·Ï‚ (DD/MM/YYYY): ").strip()
            try:
                dt_s = parse_date_input_ddmmyyyy(s)
                # CDX expects YYYYMMDDhhmmss
                from_ts = dt_s.strftime('%Y%m%d') + '000000'
                break
            except Exception:
                print("âš ï¸ ÎœÎ· Î­Î³ÎºÏ…ÏÎ· Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±. Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Ï€.Ï‡. 01/01/1999")
        # loop for valid end date
        while True:
            s = input("ğŸ”¹ Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± Î»Î®Î¾Î·Ï‚ (DD/MM/YYYY): ").strip()
            try:
                dt_e = parse_date_input_ddmmyyyy(s)
                to_ts = dt_e.strftime('%Y%m%d') + '235959'
                # ensure from <= to
                if from_ts and int(from_ts) > int(to_ts):
                    print("âš ï¸ Î— Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± Î»Î®Î¾Î·Ï‚ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ Î¼ÎµÏ„Î¬ Ï„Î·Î½ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± Î­Î½Î±ÏÎ¾Î·Ï‚.")
                    continue
                break
            except Exception:
                print("âš ï¸ ÎœÎ· Î­Î³ÎºÏ…ÏÎ· Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±. Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Ï€.Ï‡. 31/12/2015")

    # ask how many snapshots
    print("\nÎ ÏŒÏƒÎ± snapshots Î¸ÎµÏ‚ Î½Î± ÏƒÏ…Î»Î»ÎµÏ‡Î¸Î¿ÏÎ½;")
    print("1. ÎŒÎ»Î±")
    print("2. Î£Ï…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿Ï‚ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚")
    how_many = input("ğŸ‘‰ Î•Ï€Î¯Î»ÎµÎ¾Îµ (1 Î® 2): ").strip()
    max_snapshots = None
    if how_many == '2':
        while True:
            val = input("ğŸ”¢ Î Î»Î·ÎºÏ„ÏÎ¿Î»ÏŒÎ³Î·ÏƒÎµ Ï€ÏŒÏƒÎ± snapshots Î¸Î­Î»ÎµÎ¹Ï‚ (Ï€.Ï‡. 50): ").strip()
            try:
                n = int(val)
                if n > 0:
                    max_snapshots = n
                    break
            except Exception:
                pass
            print("âš ï¸ Î“ÏÎ¬ÏˆÎµ Î­Î½Î±Î½ Î¸ÎµÏ„Î¹ÎºÏŒ Î±ÎºÎ­ÏÎ±Î¹Î¿ Î±ÏÎ¹Î¸Î¼ÏŒ.")

    # build CDX query
    cdx_url = build_cdx_query(domain_path, from_ts=from_ts, to_ts=to_ts)
    print(f"\nğŸ” Î•ÏÏÏ„Î·Î¼Î± ÏƒÏ„Î¿ Wayback CDX API...\n   {cdx_url}\n")

    try:
        resp = requests.get(cdx_url, timeout=20)
        resp.raise_for_status()
        raw = resp.json()
    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ Î±Î¯Ï„Î·Î¼Î± CDX API: {e}")
        return

    if len(raw) <= 1:
        print("âš ï¸ Î¤Î¿ CDX API Î´ÎµÎ½ ÎµÏ€Î­ÏƒÏ„ÏÎµÏˆÎµ snapshots Î³Î¹Î± Ï„Î± ÎºÏÎ¹Ï„Î®ÏÎ¹Î± Î±Ï…Ï„Î¬.")
        return

    rows = raw[1:]
    # optionally limit by user-specified max_snapshots
    if max_snapshots is not None:
        rows = rows[:max_snapshots]

    total = len(rows)
    print(f"âœ… Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {total} snapshots (Î¸Î± ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ·Î¸Î¿ÏÎ½ Î»Î®ÏˆÎµÎ¹Ï‚).\n")

    all_records = []
    chunk_records = []
    chunk_index = 1

    try:
        for item in tqdm(rows, desc='Î›Î®ÏˆÎ· snapshot ÏƒÎµÎ»Î¯Î´Ï‰Î½', unit='snap'):
            try:
                timestamp, original = item
            except Exception:
                tqdm.write("âš ï¸ Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ· Î¼Î· Î±Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï… CDX entry")
                continue

            archive_url = f"https://web.archive.org/web/{timestamp}/{original}"

            try:
                html = safe_request_get(archive_url, timeout=15)
                title, content = extract_text_from_html(html)
                if not content.strip():
                    raise ValueError("ÎšÎµÎ½ÏŒ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Î¼ÎµÏ„Î¬ Ï„Î¿ parsing")

                rec = {
                    'timestamp': timestamp,
                    'original_url': original,
                    'archive_url': archive_url,
                    'title': title,
                    'content': content
                }
                all_records.append(rec)
                chunk_records.append(rec)

                # save chunk when reached CHUNK_SIZE
                if len(chunk_records) >= CHUNK_SIZE:
                    save_chunk(chunk_records, chunk_index)
                    chunk_index += 1
                    chunk_records = []

            except Exception as e:
                tqdm.write(f"âš ï¸ Î Î±ÏÎ¬Î»ÎµÎ¹ÏˆÎ· {archive_url} ({e})")
                continue

    except KeyboardInterrupt:
        print("\nâ¹ï¸ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Î´Î¹Î±ÎºÏŒÏ€Î·ÎºÎµ Î±Ï€ÏŒ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·. Î‘Ï€Î¿Î¸Î·ÎºÎµÏÎ¿Î½Ï„Î±Î¹ ÏŒÏƒÎ± ÏƒÏ…Î³ÎºÎµÎ½Ï„ÏÏÎ¸Î·ÎºÎ±Î½...")

    finally:
        # save any remaining chunk
        if chunk_records:
            save_chunk(chunk_records, chunk_index)
        # save final unified files
        save_final(all_records)
        print(f"\nÎ£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Î±ÏÏ‡ÎµÎ¯Î± Ï€Î¿Ï… ÏƒÏÎ¸Î·ÎºÎ±Î½: {len(all_records)}")


if __name__ == '__main__':
    main()

